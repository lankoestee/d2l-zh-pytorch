# Dive Into Deep Learning - PyTorch

## 说明

《动手学深度学习》的个人编写版本，虽然在代码上和[原书](https://zh.d2l.ai/index.html)是近乎一致的，但是针对原书解释不佳和其中的Python以及PyTorch的高级用法进行了一定的注释说明。同时在Notebook中也包含有对原书段落的一些高度概括，也就是将代码说明和代码运行放在了一起。

## 目录

1. 引言
2. 预备知识
- 2.1. 数据操作
- 2.2. 数据预处理
- 2.3. 线性代数
- 2.4. [微积分](src/2-4.ipynb)
- 2.5. [自动微分](src/2-4.ipynb)
- 2.6. 概率
- 2.7. 查阅文档
3. 线性神经网络
- 3.1. [线性回归](src/3-1.ipynb)
- 3.2. [线性回归的从零开始实现](src/3-2.ipynb)
- 3.3. [线性回归的简洁实现](src/3-3.ipynb)
- 3.4. [softmax回归](src/3-4.ipynb)
- 3.5. [图像分类数据集](src/3-5.ipynb)
- 3.6. [softmax回归的从零开始实现](src/3-6.ipynb)
- 3.7. [softmax回归的简洁实现](src/3-7.ipynb)
<<<<<<< HEAD
4. 多层感知机
5. 深度学习计算
=======
- 4. 多层感知机
- 5. 深度学习计算
>>>>>>> 3bc490bc38eb5bdf066eaca0a000e4fae71b53c0
- 5.1. [层和块](src/5-1.ipynb)
- 5.2. [参数管理](src/5-2.ipynb)
- 5.3. [延后初始化](src/5-3.ipynb)
- 5.4. [自定义层](src/5-4.ipynb)
- 5.5. [读写文件](src/5-5.ipynb)
- 5.6. [GPU](src/5-6.ipynb)
<<<<<<< HEAD
6. 卷积神经网络
=======
- 6. 卷积神经网络
>>>>>>> 3bc490bc38eb5bdf066eaca0a000e4fae71b53c0
- 6.1. [从全连接层到卷积](src/6-1.ipynb)
- 6.2. [图像卷积](src/6-2.ipynb)
- 6.3. [填充和步幅](src/6-3.ipynb)
- 6.4. [多输入多输出通道](src/6-4.ipynb)
- 6.5. [汇聚层](src/6-5.ipynb)
<<<<<<< HEAD
- 6.6. [卷积神经网络(LeNet)](src/6-6.ipynb)
7. 现代卷积神经网络
- 7.1. [深度卷积神经网络(AlexNet)](src/7-1.ipynb)
- 7.2. [使用块的网络(VGG)](src/7-2.ipynb)
- 7.3. [网络中的网络(NiN)](src/7-3.ipynb)
- 7.4. [含并行连结的网络(GoogleNet)](src/7-4.ipynb)
- 7.5. [批量规范化](src/7-5.ipynb)
- 7.6. [残差网络(ResNet)](src/7-6.ipynb)
- 7.7. [稠密连接网络(DenseNet)](src/7-7.ipynb)
8. 循环神经网络
- 8.1. [序列模型](src/8-1.ipynb)
- 8.2. [文本预处理](src/8-2.ipynb)
- 8.3. [语言模型和数据集](src/8-3.ipynb)
- 8.4. [循环神经网络](src/8-4.ipynb)
- 8.5. [循环神经网络的从零开始实现](src/8-5.ipynb)
- 8.6. [循环神经网络的简洁实现](src/8-6.ipynb)
- 8.7. 通过时间反向传播
9. 现代循环神经网络
10. 注意力机制
- 10.1. [注意力提示](src/10-1.ipynb)
- 10.2. [注意力汇聚：Nadaraya-Watson核回归](src/10-2.ipynb)
- 10.3. [注意力评分函数](src/10-3.ipynb)
- 10.4. Bahdanau注意力
- 10.5. [多头注意力](src/10-5.ipynb)
- 10.6. [自注意力和位置编码](src/10-6.ipynb)
- 10.7. [Transformer](src/10-7.ipynb)
11. 优化算法
12. 计算性能
13. 计算机视觉
14. 自然语言处理：预训练
15. 自然语言处理：应用
16. 附录：深度学习工具
