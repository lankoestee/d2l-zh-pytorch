{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 sofmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于类别是确定的，不是向前面的线性回归一样是一个连续的值，故采用**独热编码**解决这样的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 网络架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax回归是一种单层的神经网络\n",
    "\n",
    "![](../images/3-4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 全连接层的参数开销"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任何具有$d$个输入和$q$个输出的全连接层，其计算复杂度为$\\mathcal{O}(dq)$，这个复杂度过于高了，经过优化其成本可以减少到$\\mathcal{O}\\left(\\frac{dq}{n}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 sofmax运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax函数将为规范化的预测变换为非负数并且总和为1，并保持可导\n",
    "\n",
    "$$\n",
    "\\hat{\\bm{y}}=\\rm{softmax}(\\bm{o}) , \\quad\n",
    "\\hat{y}_j=\\frac{\\exp(o_j)}{\\sum_k\\exp(o_k)}\n",
    "$$\n",
    "\n",
    "并通过以下式子选择最有可能的类别。\n",
    "\n",
    "$$\n",
    "\\argmax_j\\hat{y}_j=\\argmax_j\\hat{o_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 小批量样本的矢量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.6 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.6.1 对数似然"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任何标签$\\bm{y}$和模型预测$\\hat{\\bm{y}}$，其损失函数为：\n",
    "\n",
    "$$\n",
    "l(\\bm{y},\\hat{\\bm{y}})=-\\sum^q_{j=1}y_j\\log\\hat{y}_j\n",
    "$$\n",
    "\n",
    "这也就是交叉熵损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.6.2 softmax及其导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单而言如下：\n",
    "\n",
    "$$\n",
    "\\partial_{o_j}l(\\bm{y},\\hat{\\bm{y}})=\\frac{\\exp(o_j)}{\\sum^q_{k=1}\\exp(o_k)}-y_j=\\rm{softmax}(\\bm{o})_j-y_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.6.3 交叉熵损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.7 信息论基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.8 模型的预测和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.9 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- softmax运算获取一个向量并将其映射为概率。\n",
    "- softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。\n",
    "- 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
